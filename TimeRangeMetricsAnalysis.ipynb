{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Range Metrics Analysis\n",
    "\n",
    "If you find an anamoly (in Logs or in use), you may want to see if there is any issue with CPU, Memory and Interface in that anamoly time-range. This notebook can be used for that purpose.\n",
    "\n",
    "Doing the same in Grafana can be time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################################\n",
    "\n",
    "Copyright 2020-21 Spirent Communications.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "  http://www.apache.org/licenses/LICENSE-2.0\n",
    "  \n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch.connection import create_ssl_context\n",
    "import ssl\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMETHEUS = 'http://10.95.197.94:30902/' #do not change, unless sure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make DF out of query json\n",
    "\n",
    "def convert_to_df(res_json):\n",
    "\n",
    "    data_list = res_json['data']['result']\n",
    "    res_df = pd.DataFrame()\n",
    "    if not data_list:\n",
    "        return res_df\n",
    "\n",
    "    # making colums\n",
    "    headers = data_list[0]\n",
    "    for data in data_list:\n",
    "        metrics = data['metric']\n",
    "        for metric in metrics.keys():\n",
    "            res_df[metric] = np.nan\n",
    "        res_df['value'] = 0\n",
    "    \n",
    "    # filling the df\n",
    "    for data in data_list:\n",
    "        metrics = data['metric']\n",
    "        metrics['value'] = data['value'][-1]\n",
    "        res_df = res_df.append(metrics, ignore_index=True)      \n",
    "\n",
    "    return res_df\n",
    "\n",
    "def convert_to_df_range(res_json):\n",
    "\n",
    "    data_list = res_json['data']['result']\n",
    "    res_df = pd.DataFrame()\n",
    "    if not data_list:\n",
    "        return res_df\n",
    "\n",
    "    # filling the df\n",
    "    for data in data_list:\n",
    "        metrics = data['metric']\n",
    "        values = np.array(data['values'])\n",
    "        for time, value in values:\n",
    "            metrics['timestamp'] = time\n",
    "            metrics['value'] = value\n",
    "            res_df = res_df.append(metrics, ignore_index=True)      \n",
    "\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to query\n",
    "\n",
    "def convert_to_timestamp(s):\n",
    "    return time.mktime(datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\").timetuple())\n",
    "\n",
    "def query_current(params={}):\n",
    "    # input: params\n",
    "    # type: dict\n",
    "    # Example: {'query': 'container_cpu_user_seconds_total'}   \n",
    "    # Output: dict, loaded json response of the query\n",
    "\n",
    "    res = requests.get(PROMETHEUS + '/api/v1/query', \n",
    "                       params=params)\n",
    "    return json.loads(res.text)\n",
    "\n",
    "\n",
    "def query_range(start, end, params={}, steps = '30s'):\n",
    "    # input: params\n",
    "    # type: dict\n",
    "    # Example: {'query': 'container_cpu_user_seconds_total'}\n",
    "    \n",
    "    # Output: dict, loaded json response of the query\n",
    "    params[\"start\"] = convert_to_timestamp(start)\n",
    "    params[\"end\"] = convert_to_timestamp(end)\n",
    "    params[\"step\"] = steps\n",
    "\n",
    "    # print(params)\n",
    "\n",
    "    res = requests.get(PROMETHEUS + '/api/v1/query_range', \n",
    "                       params=params,\n",
    "                       )\n",
    "\n",
    "    return json.loads(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU Unused Cores\n",
    "def unused_cores(start=None, end=None, node=None, steps='15s', csv=None, verbose=False):\n",
    "    \n",
    "    if csv is not None:\n",
    "        df = pd.read_csv(csv)\n",
    "        return df\n",
    "    else:\n",
    "        if start is None or end is None or node is None:\n",
    "            return \"Start, end and Node name required when fetching from prometheus\"\n",
    "        \n",
    "        params = {'query' : \"collectd_cpu_percent{exported_instance='\" + node + \"'}\"}\n",
    "\n",
    "        target_cpu_usage_range = query_range(start, end, params, steps)\n",
    "        df = convert_to_df_range(target_cpu_usage_range)\n",
    "\n",
    "        df = df.drop(['__name__', 'instance', 'job'], axis = 1)\n",
    "        groups = df.groupby(['cpu'])\n",
    "        if verbose: print(\"Unused Cores :\")\n",
    "        unused_cores = []\n",
    "        for key, item in groups:\n",
    "            curr_df = item\n",
    "            idle_row = curr_df.loc[curr_df['type'] == 'idle']\n",
    "            if idle_row['value'].iloc[0] == '100':\n",
    "                if verbose: print(\"Core:   \",key)\n",
    "                unused_cores.append(int(key))\n",
    "\n",
    "        print(\"Number of unused cores:   \", len(unused_cores))\n",
    "    return unused_cores\n",
    "\n",
    "\n",
    "#CPU fully used cores\n",
    "def fully_used_cores(start=None, end=None, node=None, steps='15s', csv=None, verbose=False):\n",
    "    \n",
    "    if csv is not None:\n",
    "        df = pd.read_csv(csv)\n",
    "        return df\n",
    "    else:\n",
    "        if start is None or end is None or node is None:\n",
    "            return \"Start, end and Node name required when fetching from prometheus\"\n",
    "        \n",
    "        params = {'query' : \"collectd_cpu_percent{exported_instance='\" + node + \"'}\"}\n",
    "\n",
    "        target_cpu_usage_range = query_range(start, end, params, steps)\n",
    "        df = convert_to_df_range(target_cpu_usage_range)\n",
    "\n",
    "        df = df.drop(['__name__', 'instance', 'job'], axis = 1)\n",
    "        groups = df.groupby(['cpu'])\n",
    "        if verbose: print(\"Fully Used Cores :\")\n",
    "        fully_used_cores = []\n",
    "        for key, item in groups:\n",
    "            curr_df = item\n",
    "            idle_row = curr_df.loc[curr_df['type'] == 'idle']\n",
    "            if idle_row['value'].iloc[0] == '0':\n",
    "                if verbose: print(\"Core:   \",key)\n",
    "                fully_used_cores.append(int(key))\n",
    "        print(\"Number of fully used cores:   \", len(fully_used_cores))\n",
    "    return fully_used_cores\n",
    "\n",
    "\n",
    "# CPU used cores plots\n",
    "def plot_used_cores(start=None, end=None, node=None, steps='15s', csv=None, verbose=False):\n",
    "    \n",
    "    if csv is not None:\n",
    "        df = pd.read_csv(csv)\n",
    "        return df\n",
    "    else:\n",
    "        if start is None or end is None or node is None:\n",
    "            return \"Start, end and Node name required when fetching from prometheus\"\n",
    "\n",
    "        params = {'query' : \"collectd_cpu_percent{exported_instance='\" + node + \"'}\"}\n",
    "\n",
    "        target_cpu_usage_range = query_range(start, end, params, steps)\n",
    "        df = convert_to_df_range(target_cpu_usage_range)\n",
    "    \n",
    "        df = df.drop(['__name__', 'instance', 'job'], axis = 1)\n",
    "        groups = df.groupby(['cpu'])\n",
    "        used_cores = []\n",
    "\n",
    "        for key, item in groups:\n",
    "            curr_df = item\n",
    "            user_row = curr_df.loc[curr_df['type'] == 'user']\n",
    "            sys_row = curr_df.loc[curr_df['type'] == 'system']\n",
    "\n",
    "\n",
    "            if np.any(sys_row != '0') or np.any(user_row != '0'):\n",
    "                used_cores.append(key)\n",
    "                type_grps = curr_df.groupby('type')\n",
    "                fig = plt.figure(figsize=(24,6), facecolor='oldlace', edgecolor='red')\n",
    "\n",
    "                for type_key, new_item in type_grps:\n",
    "\n",
    "                    if type_key == 'system':\n",
    "                        ax1 = fig.add_subplot(131)\n",
    "                        ax1.title.set_text(type_key)\n",
    "                        ax1.plot(new_item['timestamp'], new_item['value'])\n",
    "                    elif type_key == 'user':\n",
    "                        ax2 = fig.add_subplot(132)\n",
    "                        ax2.title.set_text(type_key)\n",
    "                        ax2.plot(new_item['timestamp'], new_item['value'])\n",
    "                    elif type_key == 'wait':\n",
    "                        ax3 = fig.add_subplot(133)\n",
    "                        ax3.title.set_text(type_key)\n",
    "                        ax3.plot(new_item['timestamp'], new_item['value'])\n",
    "\n",
    "                plt.suptitle('Used CPU Core {}'.format(key), fontsize=14)\n",
    "                plt.show()\n",
    "        print(\"Number of used cores:   \", len(used_cores))\n",
    "    return used_cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interface Dropped (both type 1 and 2, i.e rx and tx)\n",
    "#TODO: Change this to separate functions later\n",
    "def interface_dropped(start=None, end=None, node=None, steps='15s', csv=None, verbose=False):\n",
    "    \n",
    "    if csv is not None:\n",
    "        df = pd.read_csv(csv)\n",
    "        df_0 = df #TODO: Change this\n",
    "        df_1 = df #TODO: Change this\n",
    "    else:\n",
    "        if start is None or end is None or node is None:\n",
    "            return \"Start, end and Node name required when fetching from prometheus\"\n",
    "        \n",
    "        params = {'query' : \"collectd_interface_if_dropped_0_total{exported_instance='\" + node + \"'}\"}\n",
    "\n",
    "        interface_dropped_0 = query_range(start, end, params, steps)\n",
    "        df_0 = convert_to_df_range(interface_dropped_0)\n",
    "        \n",
    "        params = {'query' : \"collectd_interface_if_dropped_1_total{exported_instance='\" + node + \"'}\"}\n",
    "        interface_dropped_1 = query_range(start, end, params, steps)\n",
    "        df_1 = convert_to_df_range(interface_dropped_1)\n",
    "\n",
    "        \n",
    "        #df_0 : interfaces_dropped_0_df\n",
    "        df_0 = df_0.drop(['__name__', 'instance', 'job'], axis = 1)\n",
    "\n",
    "        #df_1 : interfaces_dropped_1_df\n",
    "        df_1 = df_1.drop(['__name__', 'instance', 'job'], axis = 1)\n",
    "\n",
    "        groups_0 = df_0.groupby(['interface'])\n",
    "        groups_1 = df_1.groupby(['interface'])\n",
    "\n",
    "        groups = [groups_0, groups_1]\n",
    "        dropped_interfaces= []\n",
    "        drop_type = 0\n",
    "        color = ['oldlace', 'mistyrose']\n",
    "        plot_iter = 111\n",
    "        for group in groups:\n",
    "            dropped = []\n",
    "\n",
    "            for key, item in group:\n",
    "                curr_df = item\n",
    "                if np.any(curr_df['value'] == '1'):\n",
    "                    dropped_row = curr_df.loc[curr_df['value'] == '1']\n",
    "                    dropped.append([key, dropped_row['timestamp'].iloc[0]])\n",
    "                fig = plt.figure(figsize=(24,6), facecolor=color[drop_type], edgecolor='red')\n",
    "                ax = fig.add_subplot(plot_iter)\n",
    "                ax.title.set_text(\"Interface: {}\".format(key))\n",
    "                ax.plot(item['timestamp'], item['value'])\n",
    "            dropped_interfaces.append(dropped)\n",
    "            plt.suptitle('Interfaces Drop type {}'.format(drop_type), fontsize=14)\n",
    "            plt.show()\n",
    "            drop_type += 1\n",
    "    return dropped_interfaces\n",
    "\n",
    "\n",
    "# Interface Errors (both type 1 and 2, i.e rx and tx)\n",
    "#TODO: Change this to separate functions later\n",
    "def interface_errors(start=None, end=None, node=None, steps='15s', csv=None, verbose=False):\n",
    "    \n",
    "    if csv is not None:\n",
    "        df = pd.read_csv(csv)\n",
    "        df_0 = df #TODO: Change this\n",
    "        df_1 = df #TODO: Change this\n",
    "    else:\n",
    "        if start is None or end is None or node is None:\n",
    "            return \"Start, end and Node name required when fetching from prometheus\"\n",
    "        \n",
    "        params = {'query' : \"collectd_interface_if_errors_0_total{exported_instance='\" + node + \"'}\"}\n",
    "        interfaces_errors_0 = query_range(start, end, params, steps)\n",
    "        df_0 = convert_to_df_range(interfaces_errors_0)\n",
    "        \n",
    "        params = {'query' : \"collectd_interface_if_errors_1_total{exported_instance='\" + node + \"'}\"}\n",
    "        interface_errors_1 = query_range(start, end, params, steps)\n",
    "        df_1 = convert_to_df_range(interface_errors_1)\n",
    "\n",
    "        \n",
    "        #df_0 : interfaces_errors_0_df\n",
    "        df_0 = df_0.drop(['__name__', 'instance', 'job'], axis = 1)\n",
    "\n",
    "        #df_1 : interfaces_dropped_1_df\n",
    "        df_1 = df_1.drop(['__name__', 'instance', 'job'], axis = 1)\n",
    "\n",
    "        groups_0 = df_0.groupby(['interface'])\n",
    "        groups_1 = df_1.groupby(['interface'])\n",
    "\n",
    "        groups = [groups_0, groups_1]\n",
    "        err_interfaces= []\n",
    "        err_type = 0\n",
    "        color = ['oldlace', 'mistyrose']\n",
    "        for group in groups:\n",
    "            errors = []\n",
    "\n",
    "            for key, item in group:\n",
    "                curr_df = item\n",
    "\n",
    "                if np.any(curr_df['value'] == '1'):\n",
    "                    err_row = curr_df.loc[curr_df['value'] == '1']\n",
    "                    erros.append([key, err_row['timestamp'].iloc[0]])\n",
    "\n",
    "                fig = plt.figure(figsize=(24,6), facecolor=color[err_type], edgecolor='red')\n",
    "                ax = fig.add_subplot(111)\n",
    "                ax.title.set_text(\"Interface: {}\".format(key))\n",
    "                ax.plot(item['timestamp'], item['value'])\n",
    "\n",
    "            err_interfaces.append(errors)\n",
    "            plt.suptitle('Interfaces Error type {}'.format(err_type), fontsize=14)\n",
    "            plt.show()\n",
    "            err_type += 1\n",
    "\n",
    "    return err_interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage(start=None, end=None, node=None, steps='15s', csv=None, verbose=False):\n",
    "    \n",
    "    if csv is not None:\n",
    "        df = pd.read_csv(csv)\n",
    "    else:\n",
    "        if start is None or end is None or node is None:\n",
    "            return \"Start, end and Node name required when fetching from prometheus\"\n",
    "        \n",
    "        params = {'query' : \"collectd_memory{exported_instance='\" + node + \"'} / (1024*1024*1024) \"}        \n",
    "        target_memory_usage_range = query_range(start, end, params, steps)\n",
    "        df = convert_to_df_range(target_memory_usage_range)\n",
    "        \n",
    "        df = df.drop(['instance', 'job'], axis = 1)\n",
    "        groups = df.groupby(['memory'])\n",
    "        for key, item in groups:\n",
    "            curr_df = item\n",
    "            fig = plt.figure(figsize=(24,6), facecolor='oldlace', edgecolor='red')\n",
    "            ax1 = fig.add_subplot(111)\n",
    "            ax1.title.set_text(\"Memory Type: {}\".format(key))\n",
    "            ax1.plot(item['timestamp'], item['value'])\n",
    "            plt.show()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse(timestamp, node):\n",
    "    ts = datetime.strptime(timestamp.split(',')[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "    start = ts - timedelta(seconds=10)\n",
    "    end = ts + timedelta(seconds=10)\n",
    "    \n",
    "    start = str(start)\n",
    "    end = str(end)\n",
    "    steps = '5s'\n",
    "\n",
    "    print(\"Starting Analysis from\",start,\"to\",end,'\\n\\n')\n",
    "\n",
    "    if \"node4\" in node:\n",
    "        node = 'pod12-node4'\n",
    "\n",
    "    #cpu analysis\n",
    "    print(\"=====CPU ANALYSIS=====\\n\")\n",
    "    unused = unused_cores(start, end, node, steps)\n",
    "    print(\"Unused Cores:\", unused)\n",
    "    fully_used = fully_used_cores(start, end, node, steps)\n",
    "    print(\"Fully Used Cores:\", fully_used)\n",
    "    print(\"Plotting used cores:\")\n",
    "    used_cores = plot_used_cores(start, end, node, steps)\n",
    "    \n",
    "    #interface analysis\n",
    "    print(\"=====Interfaces Dropped / Errors=====\\n\")\n",
    "    dropped_interfaces = interface_dropped(start, end, node, steps)\n",
    "    err_interfaces = interface_errors(start, end, node, steps)\n",
    "       \n",
    "    #Memory Analysis:\n",
    "    print(\"=====Memory Analysis=====\\n\")\n",
    "    mem = get_memory_usage(start, end, node, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Analyze here with the timestamp!!!\n",
    "analyze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
